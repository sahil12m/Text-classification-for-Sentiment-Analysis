{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 544 | Homework 3 \n",
    "\n",
    "### Name: Sahil Mondal\n",
    "### USC ID: 5092826451\n",
    "### Python version: 3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "# Uncomment the below if gensim error appears\n",
    "# !pip install gensim\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataset Generation\n",
    "\n",
    "We will use the Amazon reviews dataset used in HW1. Load the dataset\n",
    "and build a balanced dataset of 100K reviews along with their labels through\n",
    "random selection similar to HW1. You can store your dataset after generation\n",
    "and reuse it to reduce the computational load. For your experiments consider\n",
    "a 80%/20% training/testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahil\\AppData\\Local\\Temp\\ipykernel_33752\\3581478179.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('data.tsv', sep='\\t', on_bad_lines='skip', index_col=False)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('data.tsv', sep='\\t', on_bad_lines='skip', index_col=False)\n",
    "\n",
    "# Make an independent copy of the DataFrame\n",
    "df = data.copy()\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of above cell :\n",
    "\n",
    "It is used to load a dataset from a TSV (Tab-Separated Values) file into a Pandas DataFrame. Here's what it does:\n",
    "\n",
    "It reads the data from a TSV file named 'data.tsv' into a Pandas DataFrame, using a tab ('\\t') as the separator for the values in the file.\n",
    "\n",
    "The 'on_bad_lines' parameter is set to 'skip,' which means that if there are any lines in the file with formatting issues (bad lines), they will be skipped and not included in the DataFrame.\n",
    "\n",
    "It creates a new DataFrame 'df' as an independent copy of the loaded data.\n",
    "\n",
    "It removes rows with missing values (NaN) from 'df' using the dropna() method, effectively cleaning the dataset from rows containing missing data.\n",
    "\n",
    "The result is 'df,' a cleaned DataFrame ready for further data analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only 'Reviews' and 'Ratings' columns\n",
    "df = df[['star_rating', 'review_body']]\n",
    "\n",
    "# Create class labels: 1 for ratings 1, 2, 3; 2 for ratings 4, 5\n",
    "df['sentiment_class'] = df['star_rating'].apply(lambda x: 1 if x in [1, 2, 3] else 2)\n",
    "\n",
    "# Downsample to 50,000 reviews per class\n",
    "class1_data = resample(df[df['sentiment_class'] == 1], n_samples=50000, random_state=42)\n",
    "class2_data = resample(df[df['sentiment_class'] == 2], n_samples=50000, random_state=42)\n",
    "compressed_data = pd.concat([class1_data, class2_data])\n",
    "\n",
    "# full_data = pd.concat([compressed_data, compressed_data], ignore_index=True)\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%)\n",
    "train_df, test_df = train_test_split(compressed_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of above cell :\n",
    "\n",
    "\n",
    "The above code is used for preprocessing and splitting a dataset. Here's a breakdown:\n",
    "\n",
    "It selects only the 'star_rating' (Ratings) and 'review_body' (Reviews) columns from the DataFrame 'df' and stores them in a new DataFrame 'df'.\n",
    "\n",
    "It creates a new column 'sentiment_class' based on the 'star_rating' column. Ratings 1, 2, and 3 are assigned the class label 1, while ratings 4 and 5 are assigned the class label 2.\n",
    "\n",
    "It downsamples the data to have 50,000 samples per class, ensuring a balanced dataset by randomly selecting 50,000 samples for each sentiment class (1 and 2). The result is stored in 'compressed_data'.\n",
    "\n",
    "Optionally, there's a line that concatenates 'compressed_data' with itself, but it's commented out, so it doesn't affect the code.\n",
    "\n",
    "It splits the 'compressed_data' into training and testing sets with an 80-20 split ratio, where 80% of the data is used for training and 20% for testing. The training set is stored in 'train_df', and the testing set is stored in 'test_df'. The random_state parameter ensures reproducibility in the data split. This is a common practice for machine learning tasks to evaluate model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        star_rating                                        review_body  \\\n",
      "807022            5  Great pens.  Work well for finer detail work. ...   \n",
      "1996924           2  The head bifurcates so wide that I have to hav...   \n",
      "451173            1  some of the pen are out of ink, and ink is all...   \n",
      "1474986           1  I bought mine at Wal-Mart about three months a...   \n",
      "1811952           5  My Brother HL-2240 printer loves these labels,...   \n",
      "1160648           5  I purchased this to hang a 100 lb heavy bag fr...   \n",
      "1843674           5  We use it at our radio station and it works gr...   \n",
      "125115            1  Had to return the product twice due to it not ...   \n",
      "791052            5  I need to pull the switch( white dot) inside t...   \n",
      "1920079           3  I had an older version of this phone and was r...   \n",
      "\n",
      "         sentiment_class  \n",
      "807022                 2  \n",
      "1996924                1  \n",
      "451173                 1  \n",
      "1474986                1  \n",
      "1811952                2  \n",
      "1160648                2  \n",
      "1843674                2  \n",
      "125115                 1  \n",
      "791052                 2  \n",
      "1920079                1  \n"
     ]
    }
   ],
   "source": [
    "print(train_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Word Embedding (25 points)\n",
    "In this part the of the assignment, you will generate Word2Vec features for the dataset you generated. You can use Gensim library for this purpose. A helpful tutorial is available in the following link:\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (5 points)\n",
    "Load the pretrained “word2vec-google-news-300” Word2Vec model and learn how to extract word embeddings for your dataset. Try to check semantic similarities of the generated vectors using three examples of your own, e.g., King −M an + W oman = Queen or excellent ∼outstanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(w1, w2):\n",
    "    return np.dot(w1, w2)/(np.linalg.norm(w1)*np.linalg.norm(w2))\n",
    "\n",
    "word2vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between (king - man + woman) and queen using the pre-trained model: 0.73005176\n",
      "Similarity between outstanding and excellent using the pre-trained model: 0.5567487\n"
     ]
    }
   ],
   "source": [
    "print('Similarity between (king - man + woman) and queen using the pre-trained model:',similarity((word2vec_model['king'] - word2vec_model['man'] + word2vec_model['woman']), word2vec_model['queen']))\n",
    "\n",
    "print('Similarity between outstanding and excellent using the pre-trained model:', similarity(word2vec_model['outstanding'],word2vec_model['excellent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the above code :\n",
    "\n",
    "The code defines a similarity function using cosine similarity and utilizes the 'word2vec-google-news-300' pre-trained word embedding model. It calculates the similarity between the vectors resulting from arithmetic operations on word vectors ('king - man + woman' compared to 'queen') and between individual word vectors ('outstanding' and 'excellent') using the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Similarity between (paris - france) and berlin using the pre-trained model: 0.052674238\n"
     ]
    }
   ],
   "source": [
    "print('Example 1:')\n",
    "\n",
    "print('Similarity between (paris - france) and berlin using the pre-trained model:',similarity((word2vec_model['paris'] - word2vec_model['france']), word2vec_model['berlin']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example explores the relationship between countries and their capitals using word vectors. It calculates the similarity between the vector (country - capital) and the vector of a word representing a city. This can help you find cities that are similar in context to the relationship between a country and its capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 2:\n",
      "Similarity between (doctor - nurse) and engineer using the pre-trained model: -0.045589708\n"
     ]
    }
   ],
   "source": [
    "print('Example 2:')\n",
    "\n",
    "print('Similarity between (doctor - nurse) and engineer using the pre-trained model:',similarity((word2vec_model['doctor'] - word2vec_model['nurse']), word2vec_model['engineer']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example explores the gender associations with different professions using word vectors. It calculates the similarity between the vector (male_profession - female_profession) and the vector of a word representing a profession. This can help you identify gender biases in word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3:\n",
      "Similarity between (king - man + woman) and mother using the pre-trained model: 0.3571068\n"
     ]
    }
   ],
   "source": [
    "print('Example 3:')\n",
    "\n",
    "print('Similarity between (king - man + woman) and mother using the pre-trained model:',similarity((word2vec_model['king'] - word2vec_model['man'] + word2vec_model['woman']), word2vec_model['mother']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example performs word analogies using word vectors. It finds words that are similar to the result of a word analogy operation such as \"king - man + woman.\" This can help you discover word relationships based on semantic similarities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (20 points)\n",
    "Train a Word2Vec model using your own dataset. You will use these extracted features in the subsequent questions of this assignment. Set the embedding size to be 300 and the window size to be 13. You can also consider a minimum word count of 9. Check the semantic similarities for the same two examples in part (a). What do you conclude from comparing vectors generated by yourself and the pretrained model? Which of the Word2Vec models seems to encode semantic similarities between words better? For the rest of this assignment, use the pretrained “word2vec-google-news-300” Word2Vec features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Word2Vec(train_df.review_body.apply(lambda x:x.split()), vector_size=300,window=13,min_count=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 'queen' not present\n",
      "Similarity between outstanding and excellent using the self-trained model: 0.7948789\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Similarity between (king - man + woman) and queen using the pre-trained model:',similarity((embedding.wv['king'] - embedding.wv['man'] + embedding.wv['woman']), embedding.wv['queen']))\n",
    "except:\n",
    "    print(\"Key 'queen' not present\")\n",
    "\n",
    "print('Similarity between outstanding and excellent using the self-trained model:', similarity(embedding.wv['outstanding'],embedding.wv['excellent']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis on Self-trained model vs word2vec-google-news-300 pre-trained model :\n",
    "\n",
    "The pre-trained 'word2vec-google-news-300' is trained on a bigger corpus which makes it more robust to give out word embeddings for a much bigger vocabulary as compared to my own self-trained model which is just trained on the above dataset which makes the vocabulary poor and thus as seen in the first example above we do not find the embedding for the word 'queen'. Thus, the pre-trained model will perform better than the self-trained one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the above code :\n",
    "\n",
    "In this code, a Word2Vec model is trained on the 'review_body' text data from the 'train_df' DataFrame, converting words into 300-dimensional word embeddings. It then calculates the cosine similarity between vectors resulting from arithmetic operations on word embeddings ('king - man + woman' compared to 'queen') using the self-trained model. If the word 'queen' isn't present in the vocabulary since the vocabulary is only from the reviews that are present, it prints a message indicating so. Additionally, it calculates the similarity between word vectors for 'outstanding' and 'excellent' using the self-trained model. This code demonstrates how to train a Word2Vec model and use it to compute word vector similarities, handling the case where a word might be missing in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Simple models (20 points)\n",
    "Using the Google pre-trained Word2Vec features, train a single perceptron and an SVM model for the classification problem. For this purpose, use the average Word2Vec vectors for each review as the input feature (x =1/N∑Ni=1 Wi for a review with N words). Report your accuracy values on the testing split for these models similar to HW1, i.e., for each of perceptron and SVM models, report two accuracy values Word2Vec and TF-IDF features. What do you conclude from comparing performances for the models\n",
    "trained using the two different feature types (TF-IDF and your trained Word2Vec features) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Perceptron, Word2Vec): 0.76895\n",
      "Accuracy (Perceptron, TF-IDF): 0.81175\n",
      "Accuracy (SVM, Word2Vec): 0.79665\n",
      "Accuracy (SVM, TF-IDF): 0.8602\n"
     ]
    }
   ],
   "source": [
    "# Define a function to compute the average Word2Vec vectors for a list of reviews\n",
    "def compute_average_word2vec_vectors(reviews, model):\n",
    "    vectors = []\n",
    "    for review in reviews:\n",
    "        words = review.split()\n",
    "        review_vectors = [model[word] for word in words if word in model]\n",
    "        if review_vectors:\n",
    "            average_vector = np.mean(review_vectors, axis=0)\n",
    "            vectors.append(average_vector)\n",
    "        else:\n",
    "            # If no words in the review are in the Word2Vec model vocabulary, use a zero vector\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "    return np.array(vectors)\n",
    "\n",
    "# Compute average Word2Vec vectors for training and testing data\n",
    "X_train_word2vec = compute_average_word2vec_vectors(train_df['review_body'], word2vec_model)\n",
    "X_test_word2vec = compute_average_word2vec_vectors(test_df['review_body'], word2vec_model)\n",
    "\n",
    "# TF-IDF features (for comparison)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_df['review_body'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_df['review_body'])\n",
    "\n",
    "# Labels\n",
    "y_train = train_df['sentiment_class']\n",
    "y_test = test_df['sentiment_class']\n",
    "\n",
    "# Train Perceptron models\n",
    "perceptron_word2vec = Perceptron(eta0=0.001, random_state=42, max_iter=20000, early_stopping=True)\n",
    "perceptron_word2vec.fit(X_train_word2vec, y_train)\n",
    "perceptron_tfidf = Perceptron(eta0=0.001, random_state=42, max_iter=20000, early_stopping=True)\n",
    "perceptron_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train SVM models\n",
    "svm_word2vec = LinearSVC(random_state=42, max_iter=20000)\n",
    "svm_word2vec.fit(X_train_word2vec, y_train)\n",
    "svm_tfidf = LinearSVC(random_state=42, max_iter=20000)\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate models on testing split\n",
    "y_pred_perceptron_word2vec = perceptron_word2vec.predict(X_test_word2vec)\n",
    "y_pred_perceptron_tfidf = perceptron_tfidf.predict(X_test_tfidf)\n",
    "y_pred_svm_word2vec = svm_word2vec.predict(X_test_word2vec)\n",
    "y_pred_svm_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate accuracy for each model and feature type\n",
    "accuracy_perceptron_word2vec = accuracy_score(y_test, y_pred_perceptron_word2vec)\n",
    "accuracy_perceptron_tfidf = accuracy_score(y_test, y_pred_perceptron_tfidf)\n",
    "accuracy_svm_word2vec = accuracy_score(y_test, y_pred_svm_word2vec)\n",
    "accuracy_svm_tfidf = accuracy_score(y_test, y_pred_svm_tfidf)\n",
    "\n",
    "# Report the accuracy values\n",
    "print(\"Accuracy (Perceptron, Word2Vec):\", accuracy_perceptron_word2vec)\n",
    "print(\"Accuracy (Perceptron, TF-IDF):\", accuracy_perceptron_tfidf)\n",
    "print(\"Accuracy (SVM, Word2Vec):\", accuracy_svm_word2vec)\n",
    "print(\"Accuracy (SVM, TF-IDF):\", accuracy_svm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis :\n",
    "\n",
    "1) Perceptron :\n",
    "We observe here that this model performs better using the TF-IDF features instead of the Word2Vec features.\n",
    "2) SVM :\n",
    "We observe the similar trend here as well where the model performs better on the TF-IDF features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the above code :\n",
    "\n",
    "A function compute_average_word2vec_vectors is defined to calculate the average Word2Vec vectors for a list of reviews using a Word2Vec model. It processes each review by splitting it into words, converting words into Word2Vec vectors, and then averaging them. If a word is not present in the Word2Vec model's vocabulary, it uses a zero vector.\n",
    "\n",
    "The function is used to compute average Word2Vec vectors for both the training and testing data, storing them in X_train_word2vec and X_test_word2vec.\n",
    "\n",
    "Additionally, TF-IDF features are computed using TfidfVectorizer and stored in X_train_tfidf and X_test_tfidf for comparison.\n",
    "\n",
    "Labels for the data are stored in y_train and y_test.\n",
    "\n",
    "Two machine learning models, Perceptron and Linear Support Vector Machine (SVM), are trained using Word2Vec vectors (X_train_word2vec) and TF-IDF features (X_train_tfidf) separately. The models are trained to predict the 'sentiment_class' labels.\n",
    "\n",
    "The trained models are then evaluated on the testing data, and the predictions are stored in y_pred_perceptron_word2vec, y_pred_perceptron_tfidf, y_pred_svm_word2vec, and y_pred_svm_tfidf.\n",
    "\n",
    "The code calculates the accuracy of each model by comparing the predicted labels with the actual labels and stores the accuracy scores in variables (accuracy_perceptron_word2vec, accuracy_perceptron_tfidf, accuracy_svm_word2vec, accuracy_svm_tfidf).\n",
    "\n",
    "Finally, it prints the accuracy values for both Perceptron and SVM models, comparing Word2Vec and TF-IDF feature representations for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Feedforward Neural Networks (25 points)\n",
    "Using the Word2Vec features, train a feedforward multilayer perceptron network for classification. Consider a network with two hidden layers, each with 50 and 5 nodes, respectively. You can use cross entropy loss and your own choice for other hyperparamters, e.g., nonlinearity, number of epochs, etc. Part of getting good results is to select suitable values for these hyperparamters. You can also refer to the following tutorial to familiarize yourself: https://www.kaggle.com/mishra1993/pytorch-multi-layer-perceptron-mnist. Although the above tutorial is for image data but the concept of training an MLP is very similar to what we want to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (10 points)\n",
    "To generate the input features, use the average Word2Vec vectors similar to the “Simple models” section and train the neural network. Report accuracy values on the testing split for your MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.5721852779388428 test accuracy 0.77165\n",
      "epoch 200 loss 0.5149409770965576 test accuracy 0.7936\n",
      "epoch 300 loss 0.5035223364830017 test accuracy 0.80225\n",
      "epoch 400 loss 0.49898505210876465 test accuracy 0.8047\n",
      "epoch 500 loss 0.4964416027069092 test accuracy 0.808\n",
      "epoch 600 loss 0.49414384365081787 test accuracy 0.80945\n",
      "epoch 700 loss 0.49223682284355164 test accuracy 0.81185\n",
      "epoch 800 loss 0.49107417464256287 test accuracy 0.81365\n",
      "epoch 900 loss 0.49020877480506897 test accuracy 0.8146\n",
      "epoch 1000 loss 0.48938462138175964 test accuracy 0.81535\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Convert the Word2Vec features to PyTorch tensors\n",
    "X_train_word2vec_tensor = torch.tensor(X_train_word2vec, dtype=torch.float32)\n",
    "X_test_word2vec_tensor = torch.tensor(X_test_word2vec, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)  # Adjust labels to start from 0\n",
    "y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)  # Adjust labels to start from 0\n",
    "\n",
    "# Define the MLP model\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train_word2vec_tensor.shape[1]\n",
    "hidden_dim1 = 50\n",
    "hidden_dim2 = 5\n",
    "output_dim = 2  # Two classes (sentiment_class 1 and 2)\n",
    "model = MLPModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
    "\n",
    "# Training loop\n",
    "epochs = 1000  # You can adjust the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_word2vec_tensor)\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_logits = model(X_test_word2vec_tensor)\n",
    "            y_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "            test_loss = criterion(test_logits, y_test_tensor)\n",
    "        acc = accuracy_score(y_test_tensor.numpy(), y_pred.numpy())\n",
    "        print('epoch', epoch+1, 'loss', test_loss.item(), 'test accuracy', acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the above code :\n",
    "\n",
    "The code begins by importing necessary libraries, including PyTorch for neural network modeling and scikit-learn for accuracy score calculation.\n",
    "\n",
    "Word2Vec features and labels are converted into PyTorch tensors for training and testing data.\n",
    "\n",
    "A Multi-Layer Perceptron (MLP) neural network model is defined using the nn.Module class. It consists of three fully connected layers with ReLU activation functions between them and a softmax activation function in the final layer.\n",
    "\n",
    "The model is initialized with the specified input dimensions, hidden layer dimensions, and output dimensions (2 for sentiment classes 1 and 2).\n",
    "\n",
    "The training loop runs for a specified number of epochs, where the model is trained on the training data, and the validation loss and accuracy on the testing data are calculated.\n",
    "\n",
    "The training loop prints epoch information, test loss, and accuracy for every 100 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (15 points)\n",
    "To generate the input features, concatenate the first 10 Word2Vec vectors for each review as the input feature (x = [W T1 , ..., W T10]) and train the neural network. Report the accuracy value on the testing split for your MLP model. What do you conclude by comparing accuracy values you obtain with those obtained in the “’Simple Models” section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.565644383430481 test accuracy 0.73565\n",
      "epoch 200 loss 0.560996413230896 test accuracy 0.7367\n",
      "epoch 300 loss 0.5624485015869141 test accuracy 0.7386\n"
     ]
    }
   ],
   "source": [
    "# Create a function to generate concatenated Word2Vec features\n",
    "def generate_concatenated_word2vec(reviews, model, max_seq_length=10, embedding_dim=300):\n",
    "    concatenated_features = []\n",
    "    for review in reviews:\n",
    "        words = review.split()\n",
    "        feature = np.zeros((max_seq_length * embedding_dim,))\n",
    "        for i in range(min(len(words), max_seq_length)):\n",
    "            if words[i] in model:\n",
    "                feature[i * embedding_dim : (i + 1) * embedding_dim] = model[words[i]]\n",
    "        concatenated_features.append(feature)\n",
    "    return np.array(concatenated_features)\n",
    "\n",
    "# Use the function to generate concatenated Word2Vec features\n",
    "X_w2v_concatenated = generate_concatenated_word2vec(train_df['review_body'], word2vec_model)\n",
    "\n",
    "X_w2v_concatenated_test = generate_concatenated_word2vec(test_df['review_body'], word2vec_model)\n",
    "\n",
    "# Convert the concatenated Word2Vec features to PyTorch tensors\n",
    "X_train_word2vec_concat_tensor = torch.tensor(X_w2v_concatenated, dtype=torch.float32)\n",
    "X_test_word2vec_concat_tensor = torch.tensor(X_w2v_concatenated_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)  # Adjust labels to start from 0\n",
    "y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)  # Adjust labels to start from 0\n",
    "\n",
    "# Update the input dimension based on the concatenated vectors\n",
    "input_dim =  X_train_word2vec_concat_tensor.shape[1]  # Flattened feature dimension\n",
    "\n",
    "# Initialize the model with the updated input dimension\n",
    "model = MLPModel(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
    "\n",
    "# Training loop\n",
    "epochs = 300  # You can adjust the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_word2vec_concat_tensor)\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_logits = model(X_test_word2vec_concat_tensor)\n",
    "            y_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "            test_loss = criterion(test_logits, y_test_tensor)\n",
    "        acc = accuracy_score(y_test_tensor.numpy(), y_pred.numpy())\n",
    "        print('epoch', epoch+1, 'loss', test_loss.item(), 'test accuracy', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of concatenated first 10 Word2Vec vectors vs normal word2vec vectors on MLP :\n",
    "\n",
    "1) We can see that the MLP model performs better on the average word2vec vectors in comparison to using the first 10 concatenated vectors for each review.\n",
    "\n",
    "2) In comparison to the simple models such as SVM and Perceptron, the FFN is slightly performing better than those models for the averaged word2vec vectors. A feedforward neural network (FFN) may perform better with Word2Vec features compared to SVM and perceptron because FFNs are highly capable of learning complex, non-linear relationships within high-dimensional feature spaces, which is essential for capturing the rich semantic information encoded in Word2Vec vectors. SVM and perceptron are linear models and may struggle to exploit the full expressiveness of Word2Vec features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the above code :\n",
    "\n",
    "A function generate_concatenated_word2vec is defined to generate concatenated Word2Vec features for a list of reviews. It concatenates the Word2Vec vectors for each word in a review, up to a specified maximum sequence length. If a word is not present in the Word2Vec model's vocabulary, it uses a zero vector.\n",
    "\n",
    "The function is used to generate concatenated Word2Vec features for both the training and testing data, stored in X_w2v_concatenated and X_w2v_concatenated_test.\n",
    "\n",
    "These concatenated Word2Vec features are converted into PyTorch tensors for both training and testing datasets.\n",
    "\n",
    "The input dimension of the MLP model is updated to match the flattened feature dimension of the concatenated Word2Vec vectors.\n",
    "\n",
    "The model is reinitialized with the updated input dimension.\n",
    "\n",
    "The training loop remains similar to the previous code, but now it trains the MLP model with the concatenated Word2Vec features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Recurrent Neural Networks (30 points)\n",
    "Using the Word2Vec features, train a recurrent neural network (RNN) for classification. You can refer to the following tutorial to familiarize yourself: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) (10 points)\n",
    "Train a simple RNN for sentiment analysis. You can consider an RNN cell with the hidden state size of 10. To feed your data into our RNN, limit the maximum review length to 10 by truncating longer reviews and padding shorter reviews with a null value (0). Report accuracy values on the testing split for your RNN model. What do you conclude by comparing accuracy values you obtain with\n",
    "those obtained with feedforward neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.5763524770736694 test accuracy 0.70925\n",
      "epoch 200 loss 0.5295952558517456 test accuracy 0.73855\n",
      "epoch 300 loss 0.5144208073616028 test accuracy 0.7462\n",
      "epoch 400 loss 0.5057020783424377 test accuracy 0.7513\n",
      "epoch 500 loss 0.501423716545105 test accuracy 0.7541\n"
     ]
    }
   ],
   "source": [
    "def generate_concatenated_word2vec(reviews, model, max_seq_length=10, embedding_dim=300):\n",
    "    concatenated_features = []\n",
    "    for review in reviews:\n",
    "        words = review.split()\n",
    "        feature = np.zeros((max_seq_length, embedding_dim,))\n",
    "        for i in range(min(len(words), max_seq_length)):\n",
    "            if words[i] in model:\n",
    "                feature[i] = model[words[i]]\n",
    "        concatenated_features.append(feature)\n",
    "    return np.array(concatenated_features)\n",
    "\n",
    "# Use the function to generate concatenated Word2Vec features\n",
    "X_w2v_concatenated = generate_concatenated_word2vec(train_df['review_body'], word2vec_model)\n",
    "\n",
    "X_w2v_concatenated_test = generate_concatenated_word2vec(test_df['review_body'], word2vec_model)\n",
    "\n",
    "# Convert averaged Word2Vec vectors to PyTorch tensor\n",
    "X_train_word2vec_padded_tensor = torch.tensor(X_w2v_concatenated, dtype=torch.float32)\n",
    "X_test_word2vec_padded_tensor = torch.tensor(X_w2v_concatenated_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)  # Adjust labels to start from 0\n",
    "y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)  # Adjust labels to start from 0\n",
    "\n",
    "\n",
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        out, _ = self.rnn(x, h0.detach())\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "        return out\n",
    "    \n",
    "# Initialize the model\n",
    "input_dim = X_train_word2vec_padded_tensor.shape[2]  # Word2Vec dimension\n",
    "hidden_dim = 10\n",
    "output_dim = 2  # Two classes (sentiment_class 1 and 2)\n",
    "model = RNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
    "\n",
    "# Training loop\n",
    "epochs = 500  # You can adjust the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # y_pred = model(X_train_word2vec_padded_tensor.unsqueeze(1))\n",
    "    y_pred = model(X_train_word2vec_padded_tensor)\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # test_logits = model(X_test_word2vec_padded_tensor.unsqueeze(1))\n",
    "            test_logits = model(X_test_word2vec_padded_tensor)\n",
    "            y_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "            test_loss = criterion(test_logits, y_test_tensor)\n",
    "        acc = accuracy_score(y_test_tensor.numpy(), y_pred.numpy())\n",
    "        print('epoch', epoch+1, 'loss', test_loss.item(), 'test accuracy', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of RNN vs FFN:\n",
    "\n",
    "By looking at the testing accuracies we can say that the RNN performs better than the FFN trained on the data with first 10 concatenated vectors. It could be because RNNs can capture temporal dependencies and contextual information within sequences. Word2Vec features contain semantic relationships that benefit from sequence modeling. FFNs, lacking this sequential context, may struggle to harness the full potential of Word2Vec features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the code:\n",
    "\n",
    "The code defines a function, generate_concatenated_word2vec, which generates concatenated Word2Vec features for a list of reviews. It concatenates Word2Vec vectors for words in each review, up to a specified maximum sequence length. If a word is not present in the Word2Vec model's vocabulary, it uses a zero vector.\n",
    "\n",
    "The function is used to generate concatenated Word2Vec features for both the training and testing data, stored in X_w2v_concatenated and X_w2v_concatenated_test.\n",
    "\n",
    "These concatenated Word2Vec features are converted into PyTorch tensors for both training and testing datasets.\n",
    "\n",
    "A new RNN model is defined using the nn.Module class. This model consists of an RNN layer followed by a fully connected layer. It is designed for sequence data.\n",
    "\n",
    "The model is initialized with the specified input dimension, hidden layer dimension, and output dimension (2 for sentiment classes 1 and 2).\n",
    "\n",
    "Cross-entropy loss and the Adam optimizer are defined for training the RNN model.\n",
    "\n",
    "The training loop runs for a specified number of epochs, where the model is trained on the training data. The validation loss and accuracy on the testing data are calculated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) (10 points)\n",
    "Repeat part (a) by considering a gated recurrent unit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.6227900981903076 test accuracy 0.66375\n",
      "epoch 200 loss 0.5182005763053894 test accuracy 0.7453\n",
      "epoch 300 loss 0.49444955587387085 test accuracy 0.76025\n",
      "epoch 400 loss 0.4836018979549408 test accuracy 0.7665\n",
      "epoch 500 loss 0.477856308221817 test accuracy 0.7711\n"
     ]
    }
   ],
   "source": [
    "# Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        out, _ = self.gru(x, h0.detach())\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "        return out\n",
    "\n",
    "# Initialize the GRU model\n",
    "input_dim = X_train_word2vec_padded_tensor.shape[2]  \n",
    "hidden_dim = 10\n",
    "output_dim = 2  # Two classes (sentiment_class 1 and 2)\n",
    "model = GRUModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
    "\n",
    "# Training loop\n",
    "epochs = 500  # You can adjust the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_word2vec_padded_tensor)\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_logits = model(X_test_word2vec_padded_tensor)\n",
    "            y_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "            test_loss = criterion(test_logits, y_test_tensor)\n",
    "        acc = accuracy_score(y_test_tensor.numpy(), y_pred.numpy())\n",
    "        print('epoch', epoch+1, 'loss', test_loss.item(), 'test accuracy', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the above code :\n",
    "\n",
    "A new class, GRUModel, is defined to create a Gated Recurrent Unit (GRU) model for sentiment classification. This model consists of a GRU layer followed by a fully connected layer. The GRU is a type of recurrent neural network (RNN) designed to capture sequential patterns in data.\n",
    "\n",
    "The model is initialized with the specified input dimension, hidden layer dimension, and output dimension (2 for sentiment classes 1 and 2).\n",
    "\n",
    "Cross-entropy loss and the Adam optimizer are defined for training the GRU model.\n",
    "\n",
    "The training loop runs for a specified number of epochs, where the model is trained on the training data. The validation loss and accuracy on the testing data are calculated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) (10 points)\n",
    "Repeat part (a) by considering an LSTM unit cell.\n",
    "What do you conclude by comparing accuracy values you obtain by GRU, LSTM, and simple RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 loss 0.574582576751709 test accuracy 0.71785\n",
      "epoch 200 loss 0.5062186121940613 test accuracy 0.7553\n",
      "epoch 300 loss 0.4854887127876282 test accuracy 0.7667\n",
      "epoch 400 loss 0.4787159562110901 test accuracy 0.77135\n",
      "epoch 500 loss 0.4766538441181183 test accuracy 0.77255\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_dim).to(x.device)\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "        return out\n",
    "\n",
    "# Initialize the LSTM model\n",
    "input_dim = X_train_word2vec_padded_tensor.shape[2]\n",
    "hidden_dim = 10\n",
    "output_dim = 2  # Two classes (sentiment_class 1 and 2)\n",
    "model = LSTMModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
    "\n",
    "# Training loop\n",
    "epochs = 500  # You can adjust the number of epochs\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train_word2vec_padded_tensor)\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_logits = model(X_test_word2vec_padded_tensor)\n",
    "            y_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n",
    "            test_loss = criterion(test_logits, y_test_tensor)\n",
    "        acc = accuracy_score(y_test_tensor.numpy(), y_pred.numpy())\n",
    "        print('epoch', epoch+1, 'loss', test_loss.item(), 'test accuracy', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of RNN vs GRU vs LSTM:\n",
    "\n",
    "We can see that RNN performs worser than the other 2 models on testing accuracy. While, if we compare GRU and LSTM, they both provide comparable/similar testing accuracies (LSTM performs just slightly better).\n",
    "\n",
    "The relatively poorer performance of the basic RNN compared to GRU and LSTM with Word2Vec features can be attributed to the vanishing gradient problem. Basic RNNs have difficulty capturing long-range dependencies in sequences due to the vanishing gradient issue, which hampers their ability to leverage the rich semantic relationships embedded in Word2Vec features. GRU and LSTM architectures are designed to mitigate this problem and can better capture the nuanced semantic information, resulting in similar but better testing accuracies. LSTM, with its more sophisticated gating mechanisms, may outperform GRU slightly by capturing long-term dependencies more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the above code :\n",
    "\n",
    "A new class, LSTMModel, is defined to create a Long Short-Term Memory (LSTM) model for sentiment classification. The LSTM model consists of an LSTM layer followed by a fully connected layer. LSTMs are a type of recurrent neural network (RNN) that can capture long-range dependencies in sequential data.\n",
    "\n",
    "The model is initialized with the specified input dimension, hidden layer dimension, and output dimension (2 for sentiment classes 1 and 2).\n",
    "\n",
    "Cross-entropy loss and the Adam optimizer are defined for training the LSTM model.\n",
    "\n",
    "The training loop runs for a specified number of epochs, where the model is trained on the training data. During training, the validation loss and accuracy on the testing data are calculated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
